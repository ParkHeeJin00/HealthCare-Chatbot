{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatbot_final_model.pth  \n",
    "chatbot_model.pth  \n",
    "chatbot_model2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import urllib.request\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20938 entries, 0 to 20937\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   field     20938 non-null  object\n",
      " 1   question  20938 non-null  object\n",
      " 2   answer    20938 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 490.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "rawal_data = pd.read_csv('./data/rawal_data.csv')\n",
    "rawal_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터셋 - 80%\n",
    "train_data = rawal_data.sample(frac=0.8, random_state=2021)\n",
    "# 테스트용 데이터셋 - 20%\n",
    "test_data = rawal_data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the question is: 5134\n",
      "The minimum length of the question is: 5\n",
      "The maximum length of the answer is: 3935\n",
      "The minimum length of the answer is: 1\n"
     ]
    }
   ],
   "source": [
    "max_length = rawal_data['question'].apply(len).max()\n",
    "print(\"The maximum length of the question is:\", max_length)\n",
    "min_length = rawal_data['question'].apply(len).min()\n",
    "print(\"The minimum length of the question is:\", min_length)\n",
    "max_length_answer = rawal_data['answer'].apply(len).max()\n",
    "print(\"The maximum length of the answer is:\", max_length_answer)\n",
    "min_length_answer = rawal_data['answer'].apply(len).min()\n",
    "print(\"The minimum length of the answer is:\", min_length_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no rows with an empty answer.\n"
     ]
    }
   ],
   "source": [
    "empty_rows = rawal_data[rawal_data['answer'] == '']\n",
    "if empty_rows.empty:\n",
    "    print(\"There are no rows with an empty answer.\")\n",
    "else:\n",
    "    print(\"There are rows with an empty answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# 스페셜 토큰 정의\n",
    "Q_TKN = \"<usr>\"\n",
    "A_TKN = \"<sys>\"\n",
    "BOS = \"</s>\"\n",
    "EOS = \"</s>\"\n",
    "SENT = '<unused1>'\n",
    "PAD = \"<pad>\"\n",
    "MASK = \"<unused0>\"\n",
    "# 사전학습된 토크나이저\n",
    "TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, DF):\n",
    "        self.data = DF\n",
    "        self.questions = DF['question'].values\n",
    "        self.answers = DF['answer'].values\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.bos_token = BOS\n",
    "        self.eos_token = EOS\n",
    "        self.mask_token = MASK\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        q_tokenized = tokenizer.tokenize(self.q_token)\n",
    "        a_tokenized = tokenizer.tokenize(self.a_token)\n",
    "        sent_tokenized = tokenizer.tokenize(self.sent_token)\n",
    "        bos_tokenized = tokenizer.tokenize(self.bos_token)\n",
    "        eos_tokenized = tokenizer.tokenize(self.eos_token)\n",
    "\n",
    "        question = tokenizer.tokenize(question)\n",
    "        answer = tokenizer.tokenize(answer)\n",
    "\n",
    "        q_input = q_tokenized + question + sent_tokenized + a_tokenized\n",
    "        a_input = bos_tokenized + answer\n",
    "        a_output = answer + eos_tokenized\n",
    "\n",
    "        q_input = tokenizer.convert_tokens_to_ids(q_input)\n",
    "        a_input = tokenizer.convert_tokens_to_ids(a_input)\n",
    "        a_output = tokenizer.convert_tokens_to_ids(a_output)\n",
    "\n",
    "        return torch.tensor(q_input),torch.tensor(a_input),torch.tensor(a_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 긴문장을 기준으로 zero-padding\n",
    "def collate_fn(batch):\n",
    "    q_input, a_input, a_output = zip(*batch)\n",
    "    q_input = torch.nn.utils.rnn.pad_sequence(q_input, batch_first=True, padding_value=TOKENIZER.pad_token_id)\n",
    "    a_input = torch.nn.utils.rnn.pad_sequence(a_input, batch_first=True, padding_value=TOKENIZER.pad_token_id)\n",
    "    a_output = torch.nn.utils.rnn.pad_sequence(a_output, batch_first=True, padding_value=TOKENIZER.pad_token_id)\n",
    "\n",
    "    return q_input, a_input, a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터셋 생성\n",
    "train_dataset = ChatbotDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# 테스트용 데이터셋 생성\n",
    "test_dataset = ChatbotDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1843]) torch.Size([32, 695]) torch.Size([32, 695])\n",
      "torch.Size([32, 144]) torch.Size([32, 802]) torch.Size([32, 802])\n"
     ]
    }
   ],
   "source": [
    "# 베치크기 확인\n",
    "for q_input, a_input, a_output in train_loader:\n",
    "    print(q_input.size(), a_input.size(), a_output.size())\n",
    "    break\n",
    "for q_input, a_input, a_output in test_loader:\n",
    "    print(q_input.size(), a_input.size(), a_output.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "MODEL = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL.to(DEVICE)\n",
    "CRITERION = torch.nn.CrossEntropyLoss(ignore_index=TOKENIZER.pad_token_id)\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=1e-3)\n",
    "EPOCHS = 100\n",
    "SCHEDULER = ReduceLROnPlateau(OPTIMIZER, mode='min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (q_input, a_input, a_output) in enumerate(iterator):\n",
    "        q_input = q_input.to(device)\n",
    "        a_input = a_input.to(device)\n",
    "        a_output = a_output.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids=q_input, labels=a_output, return_dict=True)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (q_input, a_input, a_output) in enumerate(iterator):\n",
    "            q_input = q_input.to(device)\n",
    "            a_input = a_input.to(device)\n",
    "            a_output = a_output.to(device)\n",
    "            output = model(input_ids=q_input, labels=a_output, return_dict=True)\n",
    "            loss = output.loss\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = training(MODEL, train_loader, OPTIMIZER, CRITERION, DEVICE)\n",
    "    test_loss = testing(MODEL, test_loader, CRITERION, DEVICE)\n",
    "    SCHEDULER.step(test_loss)\n",
    "    train_loss.append(train_loss)\n",
    "    test_loss.append(test_loss)\n",
    "    print(f\"Epoch: {epoch+1:02}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f}\")\n",
    "    print(f\"\\tTest Loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH17_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
